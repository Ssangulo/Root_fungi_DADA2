#### GLLVM analysis 


library(gllvm)

comm <- as(otu_table(ps), "matrix")
if (taxa_are_rows(ps)) comm <- t(comm)   # samples x taxa

# Metadata
md <- as(sample_data(ps), "data.frame")

# total reads per sample
libsize <- rowSums(comm)

# store in metadata
md$libsize <- libsize
md$log_libsize <- log(libsize)

range(libsize)
all(rownames(comm) == rownames(md))

# Quick peek
head(libsize)
table(md$habitat)
table(md$site)

# keep taxa with >= 50 reads across all samples
keep <- colSums(comm) >= 50
comm_filt <- comm[, keep]

dim(comm)       # before
dim(comm_filt)  # after

#.....

#There is clear overdispersion and unbalanced design - first making a clean data prep from scratch

# ---- settings ----
min_total_reads <- 50
presence_threshold <- 30
min_prevalence_samples <- 0
habitat_ref <- "forest"
out_prefix <- "gllvm_results"

# ---- extract from phyloseq ----
stopifnot(exists("ps"), inherits(ps, "phyloseq"))

Y <- as(otu_table(ps), "matrix")
if (taxa_are_rows(ps)) Y <- t(Y)

md <- as(sample_data(ps), "data.frame")

stopifnot(nrow(Y) == nrow(md))
stopifnot(all(rownames(Y) == rownames(md)))

if (!all(c("habitat","site") %in% colnames(md))) {
  stop("metadata must contain columns 'habitat' and 'site'")
}
md$habitat <- droplevels(factor(md$habitat))
md$site    <- droplevels(factor(md$site))
if (!(habitat_ref %in% levels(md$habitat))) {
  stop(sprintf("habitat_ref='%s' not found in md$habitat levels: %s",
               habitat_ref, paste(levels(md$habitat), collapse=", ")))
}
md$habitat <- relevel(md$habitat, ref = habitat_ref)

stopifnot(all(c("site","site_elevation","Individual") %in% names(md)))
md$site           <- droplevels(factor(md$site))
md$site_elevation <- droplevels(factor(md$site_elevation))
md$Individual     <- droplevels(factor(md$Individual))


md$libsize     <- rowSums(Y)
md$log_libsize <- log(md$libsize)

message("Samples per habitat:"); print(table(md$habitat))
message("Samples per site:");    print(table(md$site))
message("Library size summary:"); print(summary(md$libsize))

# ---- filter ultra-rare taxa ----
totals <- colSums(Y)
prev   <- colSums(Y >= presence_threshold)

keep1 <- totals >= min_total_reads
keep2 <- prev   >= min_prevalence_samples
keep  <- keep1 & keep2

Yf <- Y[, keep, drop = FALSE]

message(sprintf("Taxa before: %d | after filtering: %d", ncol(Y), ncol(Yf)))
message(sprintf("Matrix sparsity after filtering: %.2f%% zeros", mean(Yf == 0) * 100))
if (ncol(Yf) == 0) stop("All taxa were filtered out. Loosen thresholds.")

# ---- design ----
Xhab <- data.frame(habitat = md$habitat)
#studyDesign <- data.frame(site = md$site)
studyDesign <- md[, c("site", "site_elevation", "Individual")]


## MODEL A: Abundance (counts)
##    NB family + offset(log libsize) AND poisson
## ---------------------------------------


#Repeating model with individual (nested within site_elevation) as random effect 
fit_nb_1 <- gllvm(
  y           = Yf,
  X           = Xhab,  
  formula     = ~ habitat,
  family      = "negative.binomial",
  offset      = md$log_libsize,
  row.eff     = ~(1|site) + (1|site_elevation:Individual),  # site + individual within site_elevation
  studyDesign = studyDesign,
  num.lv      = 1,      
  sd.errors   = FALSE,
  method      = "VA"
)

fit_poisson <- gllvm(
  y           = Yf,
  X           = Xhab,
  formula     = ~ habitat,
  family      = poisson(),
  offset      = md$log_libsize,
  row.eff     = ~(1|site),
  studyDesign = studyDesign,
  num.lv      = 0,
  sd.errors   = FALSE,
  method      = "VA"
)

# AIC and log-likelihood
AIC(fit_poisson); logLik(fit_poisson)
AIC(fit_nb); logLik(fit_nb)
AIC(fit_nb_1); logLik(fit_nb_1)


# Compare models directly
anova(fit_poisson, fit_nb)

#### NB outperforms poisson in AIC and loglikehood

######## 

# PRIMARY ANALYSIS 

library(dplyr)
library(stringr)

## 0) full coef table 
coef_nb <- as.data.frame(coef(fit_nb_1)$Xcoef) %>%
  tibble::rownames_to_column("taxon")

## 1) join taxonomy (full), keep only needed ranks
tax <- as.data.frame(tax_table(ps)) %>%
  tibble::rownames_to_column("taxon")

annot_raw <- coef_nb %>%
  left_join(tax %>% select(taxon, Genus, Family, Order, Class, Phylum), by = "taxon")

## 2) clean prefixes/suffixes like "o__" and " Family"/" Order"
clean_tax <- function(x) {
  x <- as.character(x)
  x <- gsub("^\\w__","", x)                         # drop rank prefixes (g__, o__, etc.)
  x <- gsub(" (Family|Order|Class|Phylum)$","", x)  # drop trailing rank words if present
  ifelse(is.na(x) | x == "", "Unassigned", x)
}
annot_raw <- annot_raw %>%
  mutate(
    Genus = clean_tax(Genus),
    Order = clean_tax(Order)
  )

## 3) sanity: ensure coef columns are numeric
stopifnot(is.numeric(annot_raw$habitatparamo))
if ("habitatsubparamo" %in% names(annot_raw)) stopifnot(is.numeric(annot_raw$habitatsubparamo))

## 4) per-OTU abundance (weights) â€” use colSums, not rowSums
otu_abund <- colSums(Yf)  # Yf: samples x taxa used in the model
annot_raw$w_abund <- pmax(1, otu_abund[match(annot_raw$taxon, names(otu_abund))])

## 5) GENUS-LEVEL summary across ALL OTUs
min_n <- 5
genus_summary <- annot_raw %>%
  group_by(Order, Genus) %>%
  summarise(
    n_OTUs   = n(),
    mean_beta_paramo = mean(habitatparamo, na.rm = TRUE),
    q90_beta_paramo  = quantile(habitatparamo, 0.90, na.rm = TRUE),
    max_beta_paramo  = max(habitatparamo, na.rm = TRUE),
    wmean_beta_paramo = weighted.mean(habitatparamo, w = w_abund, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(n_OTUs >= min_n) %>%
  arrange(desc(q90_beta_paramo), desc(wmean_beta_paramo))

## 6) ORDER-LEVEL summary across ALL OTUs
order_summary <- annot_raw %>%
  group_by(Order) %>%
  summarise(
    n_OTUs   = n(),
    median_beta_paramo = median(habitatparamo, na.rm = TRUE),
    q90_beta_paramo    = quantile(habitatparamo, 0.90, na.rm = TRUE),
    max_beta_paramo    = max(habitatparamo, na.rm = TRUE),
    wmean_beta_paramo  = weighted.mean(habitatparamo, w = w_abund, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  filter(n_OTUs >= min_n) %>%
  arrange(desc(q90_beta_paramo), desc(wmean_beta_paramo))

# peek
head(genus_summary, 15)
head(order_summary, 15)

#######









# Poisson
coef_poisson <- as.data.frame(coef(fit_poisson)$Xcoef) %>%
  tibble::rownames_to_column("taxon")

# NB
coef_nb <- as.data.frame(coef(fit_nb_1)$Xcoef) %>%
  tibble::rownames_to_column("taxon")

# See what predictors are available
colnames(coef_nb)

top_paramo <- coef_nb %>%
  arrange(desc(habitatparamo)) %>%
  slice_head(n = 30)

top_subparamo <- coef_nb %>%
  arrange(desc(habitatsubparamo)) %>%
  slice_head(n = 30)


#Assigning taxonomy
tax <- as.data.frame(tax_table(ps)) %>% tibble::rownames_to_column("taxon")
annotated_paramo <- left_join(top_paramo, tax, by="taxon")
annotated_subparamo <- left_join(top_subparamo, tax, by="taxon")


#Making list with order and genus columns 

paramo_summary <- paramo_with_tax %>%
  group_by(Order, Genus) %>%
  summarise(
    n_OTUs = n(),
    mean_beta = mean(habitatparamo, na.rm = TRUE),
    max_beta  = max(habitatparamo, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_beta))

subparamo_summary <- subparamo_with_tax %>%
  group_by(Order, Genus) %>%
  summarise(
    n_OTUs = n(),
    mean_beta = mean(habitatsubparamo, na.rm = TRUE),
    max_beta  = max(habitatsubparamo, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  arrange(desc(mean_beta))

head(paramo_summary, 15)
head(subparamo_summary, 15)



###Collapsing at order level --- not usefull... mean coefficients downweight specialist OTUs within each order...

## 1) Join coefficients with taxonomy (no cleaning)
annot_raw <- coef_nb %>%
  left_join(tax, by = "taxon")

## 2) Find the Order column name as-is (case-insensitive match of "order")
tax_cols <- colnames(tax)
col_order <- tax_cols[grep("(?i)^order$", tax_cols, perl = TRUE)]
if (length(col_order) == 0) stop("Could not find an 'Order' column in tax_table(ps).")
col_order <- col_order[1]  # take the first match

## 3) Use raw Order (keep prefixes), replace NA/blank with "Unassigned"
annot_raw <- annot_raw %>%
  mutate(
    Order_raw = .data[[col_order]],
    Order_raw = ifelse(is.na(Order_raw) | Order_raw == "", "Unassigned", Order_raw)
  )

## 4) Collapse to Order level with simple robust summaries
min_OTUs_per_order <- 5  # adjust 

order_summary_raw <- annot_raw %>%
  group_by(Order_raw) %>%
  summarise(
    n_OTUs           = n(),
    median_paramo    = median(habitatparamo, na.rm = TRUE),
    median_subparamo = median(habitatsubparamo, na.rm = TRUE),
    mean_paramo      = mean(habitatparamo, na.rm = TRUE),
    mean_subparamo   = mean(habitatsubparamo, na.rm = TRUE),
    q75_paramo       = quantile(habitatparamo, 0.75, na.rm = TRUE),
    q75_subparamo    = quantile(habitatsubparamo, 0.75, na.rm = TRUE),
    q90_paramo       = quantile(habitatparamo, 0.90, na.rm = TRUE),
    q90_subparamo    = quantile(habitatsubparamo, 0.90, na.rm = TRUE),
    .groups = "drop"
  ) %>%
  mutate(delta_paramo_vs_sub = median_paramo - median_subparamo) %>%
  filter(n_OTUs >= min_OTUs_per_order)

## 5) Top orders (no cleaning, prefixes intact)
top_orders_paramo_raw <- order_summary_raw %>%
  arrange(desc(median_paramo)) %>%
  select(Order_raw, n_OTUs, median_paramo, median_subparamo, delta_paramo_vs_sub, q90_paramo) %>%
  slice_head(n = 20)

top_orders_subparamo_raw <- order_summary_raw %>%
  arrange(desc(median_subparamo)) %>%
  select(Order_raw, n_OTUs, median_paramo, median_subparamo, delta_paramo_vs_sub, q90_subparamo) %>%
  slice_head(n = 20)

## 6) Peek + export
print(top_orders_paramo_raw)
print(top_orders_subparamo_raw)

#write_csv(order_summary_raw,         "gllvm_order_summary_raw.csv")
#write_csv(top_orders_paramo_raw,     "gllvm_top_orders_paramo_raw.csv")
#write_csv(top_orders_subparamo_raw,  "gllvm_top_orders_subparamo_raw.csv")



#Paramo enriched orders analysis with specialist taxa

order_summary_alt <- annot_raw %>%
  group_by(Order_raw) %>%
  summarise(
    n_OTUs = n(),
    median_paramo = median(habitatparamo, na.rm = TRUE),
    q90_paramo    = quantile(habitatparamo, 0.90, na.rm = TRUE),
    max_paramo    = max(habitatparamo, na.rm = TRUE),
    weighted_mean_paramo = weighted.mean(habitatparamo, w = colSums(Yf)[match(taxon, rownames(Yf))], na.rm = TRUE)
  ) %>%
  arrange(desc(q90_paramo))

## Top 15 orders by q90 (captures the upper tail / specialists)
order_summary_alt %>%
  arrange(desc(q90_paramo)) %>%
  slice_head(n = 15)

## Top 15 orders by weighted mean (abundance-driven signal)
order_summary_alt %>%
  arrange(desc(weighted_mean_paramo)) %>%
  slice_head(n = 15)

## Top 15 orders by max (most extreme specialists)
order_summary_alt %>%
  arrange(desc(max_paramo)) %>%
  slice_head(n = 15)


